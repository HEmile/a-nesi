N: 8
batch_size: 16
amt_samples: 50
nrm_lr: 0.0000014
perception_lr: 0.0032
nrm_loss: 'mse'
perception_loss: 'log-q'
percept_loss_pref: 1
policy: 'off'
hidden_size: 1720
K_beliefs: 100
dirichlet_lr: 0.04
dirichlet_iters: 60
dirichlet_init: 0.01
dirichlet_L2: 1000
layers: 2
DEBUG: True
test: False
